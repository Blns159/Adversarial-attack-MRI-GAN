{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI Image Generation with WGAN-GP\n",
    "\n",
    "This notebook implements a WGAN-GP (Wasserstein GAN with Gradient Penalty) to generate synthetic MRI images. We'll:\n",
    "\n",
    "## Steps:\n",
    "1. Install required packages\n",
    "2. Load and preprocess 2D MRI data\n",
    "3. Train the DCGAN model\n",
    "4. Generate new MRI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-02T04:04:06.339966Z",
     "iopub.status.busy": "2025-06-02T04:04:06.339636Z",
     "iopub.status.idle": "2025-06-02T04:04:14.933727Z",
     "shell.execute_reply": "2025-06-02T04:04:14.932943Z",
     "shell.execute_reply.started": "2025-06-02T04:04:06.339939Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.autograd import Variable, grad\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T04:05:46.848064Z",
     "iopub.status.busy": "2025-06-02T04:05:46.847182Z",
     "iopub.status.idle": "2025-06-02T04:06:03.165614Z",
     "shell.execute_reply": "2025-06-02T04:06:03.164912Z",
     "shell.execute_reply.started": "2025-06-02T04:05:46.84801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define data directory\n",
    "data_dir = \"PATH/TO/DATA/FOLDER\"\n",
    "png_files = glob.glob(f\"{data_dir}/*.png\")\n",
    "print(f\"Found {len(png_files)} PNG images\")\n",
    "\n",
    "# Transformations for PNG images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale\n",
    "    transforms.Resize((128, 128)),                # Resize to 128x128\n",
    "    transforms.ToTensor(),                        # Convert to tensor [0,1]\n",
    "    transforms.Normalize([0.5], [0.5])            # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Function to load and preprocess PNG image\n",
    "def load_and_preprocess_png(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess a 2D PNG image\n",
    "    \"\"\"\n",
    "    image = Image.open(file_path).convert(\"RGB\")\n",
    "    return transform(image)\n",
    "\n",
    "# Load all images\n",
    "print(\"Loading and preprocessing PNG images...\")\n",
    "slices = []\n",
    "for file_path in tqdm(png_files):\n",
    "    try:\n",
    "        slice_tensor = load_and_preprocess_png(file_path)\n",
    "        slices.append(slice_tensor)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Create tensor dataset\n",
    "dataset = TensorDataset(torch.stack(slices))\n",
    "print(f\"Dataset created with {len(dataset)} images\")\n",
    "\n",
    "# Visualize some slices\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(min(5, len(slices))):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.imshow(slices[i].squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Generator and Discriminator Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T04:19:25.498075Z",
     "iopub.status.busy": "2025-06-02T04:19:25.497506Z",
     "iopub.status.idle": "2025-06-02T04:19:25.760638Z",
     "shell.execute_reply": "2025-06-02T04:19:25.760014Z",
     "shell.execute_reply.started": "2025-06-02T04:19:25.498052Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "img_size = 128\n",
    "channels = 1\n",
    "latent_dim = 128\n",
    "batch_size = 64\n",
    "n_epochs = 750\n",
    "n_critic = 5\n",
    "lambda_gp = 10\n",
    "lr = 0.0002              \n",
    "b1 = 0.1\n",
    "b2 = 0.999\n",
    "sample_interval = 2000\n",
    "\n",
    "img_shape = (channels, img_size, img_size)\n",
    "\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(latent_dim, 128, normalize=False),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            *block(512, 1024),\n",
    "            nn.Linear(1024, int(np.prod(img_shape))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        img = self.model(z)\n",
    "        img = img.view(img.shape[0], *img_shape)\n",
    "        return img\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_shape)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity\n",
    "\n",
    "# Initialize generator and discriminator\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Create output directory for generated images\n",
    "os.makedirs(\"generated_images\", exist_ok=True)\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Function to compute gradient penalty\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1).to(device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(torch.ones(real_samples.shape[0], 1).to(device), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T04:19:29.0528Z",
     "iopub.status.busy": "2025-06-02T04:19:29.052208Z",
     "iopub.status.idle": "2025-06-02T04:39:57.137198Z",
     "shell.execute_reply": "2025-06-02T04:39:57.136165Z",
     "shell.execute_reply.started": "2025-06-02T04:19:29.052779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "batches_done = 0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (real_imgs,) in enumerate(dataloader):\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = real_imgs.to(device)\n",
    "\n",
    "        # -----------------\n",
    "        # Train Discriminator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(real_imgs.shape[0], latent_dim).to(device)\n",
    "\n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z)\n",
    "\n",
    "        # Real images\n",
    "        real_validity = discriminator(real_imgs)\n",
    "        # Fake images\n",
    "        fake_validity = discriminator(fake_imgs.detach())\n",
    "        # Gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, real_imgs, fake_imgs.detach())\n",
    "        # Adversarial loss\n",
    "        d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        # Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        if i % n_critic == 0:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Generate a batch of images\n",
    "            fake_imgs = generator(z)\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            # Train on fake images\n",
    "            fake_validity = discriminator(fake_imgs)\n",
    "            g_loss = -torch.mean(fake_validity)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Save losses for plotting\n",
    "            g_losses.append(g_loss.item())\n",
    "            d_losses.append(d_loss.item())\n",
    "\n",
    "            # Print progress\n",
    "            if i % 10 == 0:\n",
    "                print(\n",
    "                    f\"[Epoch {epoch}/{n_epochs}] \"\n",
    "                    f\"[Batch {i}/{len(dataloader)}] \"\n",
    "                    f\"[D loss: {d_loss.item():.4f}] \"\n",
    "                    f\"[G loss: {g_loss.item():.4f}]\"\n",
    "                )\n",
    "\n",
    "            # Save generated images\n",
    "            if batches_done % sample_interval == 0:\n",
    "                save_image(fake_imgs.data[:25], f\"generated_images/{batches_done}.png\", \n",
    "                           nrow=5, normalize=True)\n",
    "                \n",
    "            batches_done += n_critic\n",
    "\n",
    "# Save the model\n",
    "torch.save(generator.state_dict(), \"mri_wgan_gp_generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"mri_wgan_gp_discriminator.pth\")\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T04:47:14.806286Z",
     "iopub.status.busy": "2025-06-02T04:47:14.805766Z",
     "iopub.status.idle": "2025-06-02T04:47:15.304294Z",
     "shell.execute_reply": "2025-06-02T04:47:15.303497Z",
     "shell.execute_reply.started": "2025-06-02T04:47:14.806262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot the training losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(g_losses, label=\"Generator loss\")\n",
    "plt.plot(d_losses, label=\"Discriminator loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Losses\")\n",
    "plt.savefig(\"loss_plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T04:47:19.743927Z",
     "iopub.status.busy": "2025-06-02T04:47:19.743638Z",
     "iopub.status.idle": "2025-06-02T04:47:21.64731Z",
     "shell.execute_reply": "2025-06-02T04:47:21.646435Z",
     "shell.execute_reply.started": "2025-06-02T04:47:19.743906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the trained generator\n",
    "generator = Generator().to(device)\n",
    "generator.load_state_dict(torch.load(\"mri_wgan_gp_generator.pth\"))\n",
    "generator.eval()\n",
    "\n",
    "# Generate new images\n",
    "num_images = 16\n",
    "z = torch.randn(num_images, latent_dim).to(device)\n",
    "with torch.no_grad():\n",
    "    generated_images = generator(z)\n",
    "\n",
    "# Display generated images\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(num_images):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    # Convert from tensor (-1,1) range to numpy (0,1) for display\n",
    "    img = (generated_images[i].cpu().squeeze().numpy() + 1) / 2\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"generated_mri_samples.png\")\n",
    "plt.show()\n",
    "\n",
    "print(\"New MRI images have been successfully generated!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7519399,
     "sourceId": 11958945,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
